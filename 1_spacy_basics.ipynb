{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import spacy\r\n",
    "nlp = spacy.load('en_core_web_sm')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "spacy.__version__"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'3.0.6'"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tokenization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tokenization is the task of splitting a text into meaningful segments called tokens. The input to the tokenizer is a unicode text and the output is a Doc object.\r\n",
    "\r\n",
    "A Doc is a sequence of Token objects. Each Doc consists of individual tokens, and we can iterate over them."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "doc = nlp(\"Apple is looking at buyig U.K. startup for $1 billion\")\r\n",
    "for token in doc:\r\n",
    "    print(token.text)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Apple\n",
      "is\n",
      "looking\n",
      "at\n",
      "buyig\n",
      "U.K.\n",
      "startup\n",
      "for\n",
      "$\n",
      "1\n",
      "billion\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "doc = nlp(\"Apple isn't looking at buyig U.K. startup for $1 billion\")\r\n",
    "for token in doc:\r\n",
    "    print(token.text)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Apple\n",
      "is\n",
      "n't\n",
      "looking\n",
      "at\n",
      "buyig\n",
      "U.K.\n",
      "startup\n",
      "for\n",
      "$\n",
      "1\n",
      "billion\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Lemmatization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "A work-related to tokenization, lemmatization is the method of decreasing the word to its base form, or origin form. This reduced form or root word is called a lemma.\r\n",
    "\r\n",
    "For example, organizes, organized and organizing are all forms of organize. Here, organize is the lemma.\r\n",
    "\r\n",
    "Lemmatization is necessary because it helps to reduce the inflected forms of a word so that they can be analyzed as a single item. It can also help you normalize the text."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "for token in doc:\r\n",
    "    print(token.text, token.lemma_)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Apple Apple\n",
      "is be\n",
      "n't n't\n",
      "looking look\n",
      "at at\n",
      "buyig buyig\n",
      "U.K. U.K.\n",
      "startup startup\n",
      "for for\n",
      "$ $\n",
      "1 1\n",
      "billion billion\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part-of-speech tagging"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Part of speech tagging is the process of assigning a POS tag to each token depending on its usage in the sentence."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "for token in doc:\r\n",
    "    print(f'{token.text:{15}} {token.lemma_:{15}} {token.pos_:{10}} {token.is_stop}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Apple           Apple           PROPN      False\n",
      "is              be              AUX        True\n",
      "n't             n't             PART       True\n",
      "looking         look            VERB       False\n",
      "at              at              ADP        True\n",
      "buyig           buyig           NOUN       False\n",
      "U.K.            U.K.            PROPN      False\n",
      "startup         startup         NOUN       False\n",
      "for             for             ADP        True\n",
      "$               $               SYM        False\n",
      "1               1               NUM        False\n",
      "billion         billion         NUM        False\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dependency Parsing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dependency parsing is the process of extracting the dependency parse of a sentence to represent its grammatical structure. It defines the dependency relationship between headwords and their dependents. The head of a sentence has no dependency and is called the root of the sentence. The verb is usually the head of the sentence. All other words are linked to the headword.\r\n",
    "\r\n",
    "Noun chunks are “base noun phrases” – flat phrases that have a noun as their head.To get the noun chunks in a document, simply iterate over Doc.noun_chunks."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "for chunk in doc.noun_chunks:\r\n",
    "    print(f'{chunk.text:{30}} {chunk.root.text:{15}} {chunk.root.dep_}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Apple                          Apple           nsubj\n",
      "buyig                          buyig           pobj\n",
      "U.K.                           U.K.            nsubj\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Named Entity Recognition"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Named Entity Recognition (NER) is the process of locating named entities in unstructured text and then classifying them into pre-defined categories, such as person names, organizations, locations, monetary values, percentages, time expressions, and so on.\r\n",
    "\r\n",
    "It is used to populate tags for a set of documents in order to improve the keyword search. Named entities are available as the ents property of a Doc."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "for ent in doc.ents:\r\n",
    "    print(ent.text, ent.label_)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Apple ORG\n",
      "U.K. GPE\n",
      "$1 billion MONEY\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "dir(doc)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['_',\n",
       " '__bytes__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pyx_vtable__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " '_bulk_merge',\n",
       " '_get_array_attrs',\n",
       " '_py_tokens',\n",
       " '_realloc',\n",
       " '_vector',\n",
       " '_vector_norm',\n",
       " 'cats',\n",
       " 'char_span',\n",
       " 'copy',\n",
       " 'count_by',\n",
       " 'doc',\n",
       " 'ents',\n",
       " 'extend_tensor',\n",
       " 'from_array',\n",
       " 'from_bytes',\n",
       " 'from_dict',\n",
       " 'from_disk',\n",
       " 'from_docs',\n",
       " 'get_extension',\n",
       " 'get_lca_matrix',\n",
       " 'has_annotation',\n",
       " 'has_extension',\n",
       " 'has_unknown_spaces',\n",
       " 'has_vector',\n",
       " 'is_nered',\n",
       " 'is_parsed',\n",
       " 'is_sentenced',\n",
       " 'is_tagged',\n",
       " 'lang',\n",
       " 'lang_',\n",
       " 'mem',\n",
       " 'noun_chunks',\n",
       " 'noun_chunks_iterator',\n",
       " 'remove_extension',\n",
       " 'retokenize',\n",
       " 'sentiment',\n",
       " 'sents',\n",
       " 'set_ents',\n",
       " 'set_extension',\n",
       " 'similarity',\n",
       " 'spans',\n",
       " 'tensor',\n",
       " 'text',\n",
       " 'text_with_ws',\n",
       " 'to_array',\n",
       " 'to_bytes',\n",
       " 'to_dict',\n",
       " 'to_disk',\n",
       " 'to_json',\n",
       " 'to_utf8_array',\n",
       " 'user_data',\n",
       " 'user_hooks',\n",
       " 'user_span_hooks',\n",
       " 'user_token_hooks',\n",
       " 'vector',\n",
       " 'vector_norm',\n",
       " 'vocab']"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sentence Segmentation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sentence Segmentation is the process of locating the start and end of sentences in a given text. This allows you to you divide a text into linguistically meaningful units.SpaCy uses the dependency parse to determine sentence boundaries. In spaCy, the sents property is used to extract sentences."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "for sent in doc.sents:\r\n",
    "    print(sent)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Apple isn't looking at buyig U.K. startup for $1 billion\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "doc1 = nlp(\"Welcome to ML tutorials. Thanks for watching. Please like and subscribe\")\r\n",
    "for sent in doc1.sents:\r\n",
    "    print(sent)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Welcome to ML tutorials.\n",
      "Thanks for watching.\n",
      "Please like and subscribe\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "doc1 = nlp(\"Welcome to.*.ML tutorials.*.Thanks for watching\")\r\n",
    "for sent in doc1.sents:\r\n",
    "    print(sent)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Welcome to.*.ML tutorials.*.Thanks for watching\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "From the above example our sentence segmentation process fail to detect the sentence boundries due to delimiters. In such cases we write our own customize rules to detect sentence boundry based on delimiters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "text = 'Welcome to KGP Talkie...Thanks...Like and Subscribe!'\r\n",
    "doc = nlp(text)\r\n",
    "for sent in doc.sents:\r\n",
    "    print(sent)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Welcome to KGP Talkie...\n",
      "Thanks...Like and Subscribe!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "nlp.pipeline"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x1b01d554c20>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x1b01d1b49a0>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x1b01d3a3b80>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x1b01d55f3a0>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x1b01e839940>),\n",
       " ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x1b01e84a1c0>),\n",
       " ('sentencizer', <spacy.pipeline.sentencizer.Sentencizer at 0x1b02044b780>)]"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "def set_rule(doc):\r\n",
    "    for token in doc[:-1]:\r\n",
    "        if token.text == '...':\r\n",
    "            doc[token.i + 1].is_sent_start = True\r\n",
    "    return doc"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('venv_datascience': venv)"
  },
  "interpreter": {
   "hash": "cd2eb60fb626c8b3c62532dabbad0328d4ef163364388f62e4351269bf2999f7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}